# How can we run a llm locally?
## Overview
As I start to dive into AI I have wanted to understand what are the different options for accessing and or running an LLM.
Currently I am writing this and will be running some experimentation on a 2019 Macbook Pro with 16gb and an 2.8GHz i7 CPU and Intel Iris Plus 655 GPU with of 1536 MB of vram. I purchased this macbook for $200 on Craigslist in June of 2025. In the future I might explore purchasing one of the new M4 based Macbook if I run into too many limitations in testing and experimentation. 

Please be aware that there might be factual or nomenclature errors and misconceptions throughout my research documentation. I will do my best to correct them as I find them.

# Questions
## What are the fundamental software and hardware requirements to running an LLM locally?

To run an LLM model locally we do require some basic hardware. CPU, Memory, Storage, and possibly a GPU. 
Looking into the optionality of a GPU it seems that it is possible to do your inference on a CPU instead of a GPU.

From a software standpoint we will need at the minimum an Inference Engine like llama.cpp or Ollama?

## What is PyTorch vs fastai vs etc?

----

## What are the performance differences between performing inference on a GPU vs a CPU?

## What are the options for running an LLM in the cloud on a VPS or managed service?

## What are the trade offs for running on subpar hardware?

## What are the benefits of running an LLM locally vs using a 1st party hosted llm solution?

## What are the difference in requirements between different models? Deepseek vs LLAMA, etc.

# Experiments
## Attempting to run a small LLM on a 2019 Macbook Pro with 16 GB of RAM
### Attempt #1
Software: 