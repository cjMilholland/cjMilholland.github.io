# How can we run a llm locally?
## Overview
As I start to dive into AI I have wanted to understand what are the different options for accessing and or running an LLM.

# Questions
## What are the fundamental software and hardware requirements to running an LLM locally?

## What are the options for running an LLM in the cloud on a VPS or managed service?

## What are the trade offs for running on subpar hardware?

## What are the benefits of running an LLM locally vs using a 1st party hosted llm solution?

## What are the difference in requirements between different models? Deepseek vs LLAMA, etc.

# Conclusion